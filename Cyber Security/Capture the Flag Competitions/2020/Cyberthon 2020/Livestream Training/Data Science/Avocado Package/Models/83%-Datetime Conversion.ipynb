{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avocado Data Analysis Notebook\n",
    "ALT-TAB LABS LLP &copy; 2020 All Rights Reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['75%.py',\n '80% - Copy.ipynb',\n '80%.ipynb',\n 'avocado-submission.csv',\n 'avocado-test.csv',\n 'avocado-train.csv',\n 'avocado_notebook_other_models.ipynb',\n 'pipeline test.ipynb',\n 'pyctfsglib.py',\n '__pycache__']"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show files in current directory\n",
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DataBatch</th>\n      <th>Date</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n      <th>type</th>\n      <th>region</th>\n      <th>AveragePrice</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>2016-11-06</td>\n      <td>183542.31</td>\n      <td>98949.98</td>\n      <td>22891.61</td>\n      <td>95.00</td>\n      <td>61605.72</td>\n      <td>43571.99</td>\n      <td>17499.01</td>\n      <td>534.72</td>\n      <td>conventional</td>\n      <td>NewOrleansMobile</td>\n      <td>1.49</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24</td>\n      <td>2017-07-16</td>\n      <td>224434.92</td>\n      <td>42951.31</td>\n      <td>120360.02</td>\n      <td>131.85</td>\n      <td>60991.74</td>\n      <td>53141.81</td>\n      <td>3621.04</td>\n      <td>4228.89</td>\n      <td>conventional</td>\n      <td>HarrisburgScranton</td>\n      <td>1.38</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51</td>\n      <td>2015-01-04</td>\n      <td>3846.69</td>\n      <td>1500.15</td>\n      <td>938.35</td>\n      <td>0.00</td>\n      <td>1408.19</td>\n      <td>1071.35</td>\n      <td>336.84</td>\n      <td>0.00</td>\n      <td>organic</td>\n      <td>Atlanta</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>2015-07-26</td>\n      <td>91825.07</td>\n      <td>1679.28</td>\n      <td>45615.48</td>\n      <td>741.77</td>\n      <td>43788.54</td>\n      <td>43788.54</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>conventional</td>\n      <td>BuffaloRochester</td>\n      <td>1.39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50</td>\n      <td>2015-01-11</td>\n      <td>54644.32</td>\n      <td>1491.88</td>\n      <td>33759.12</td>\n      <td>1325.17</td>\n      <td>18068.15</td>\n      <td>12165.94</td>\n      <td>5902.21</td>\n      <td>0.00</td>\n      <td>conventional</td>\n      <td>Pittsburgh</td>\n      <td>1.54</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    DataBatch        Date  TotalVolume      4046       4225     4770  \\\nid                                                                     \n0           7  2016-11-06    183542.31  98949.98   22891.61    95.00   \n1          24  2017-07-16    224434.92  42951.31  120360.02   131.85   \n2          51  2015-01-04      3846.69   1500.15     938.35     0.00   \n3          22  2015-07-26     91825.07   1679.28   45615.48   741.77   \n4          50  2015-01-11     54644.32   1491.88   33759.12  1325.17   \n\n    TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\nid                                                              \n0    61605.72   43571.99   17499.01      534.72  conventional   \n1    60991.74   53141.81    3621.04     4228.89  conventional   \n2     1408.19    1071.35     336.84        0.00       organic   \n3    43788.54   43788.54       0.00        0.00  conventional   \n4    18068.15   12165.94    5902.21        0.00  conventional   \n\n                region  AveragePrice  \nid                                    \n0     NewOrleansMobile          1.49  \n1   HarrisburgScranton          1.38  \n2              Atlanta          1.76  \n3     BuffaloRochester          1.39  \n4           Pittsburgh          1.54  "
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csvs\n",
    "import pandas as pd\n",
    "df = pd.read_csv('avocado-train.csv', index_col='id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "training split:  8558 ; test split:  4216\n"
    }
   ],
   "source": [
    "# Convert strings to numbers\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "c = [\"DataBatch\",'TotalVolume',\"4046\",\"4225\",\"4770\",\"TotalBags\",\"SmallBags\",\"LargeBags\",\"XLargeBags\",\"AveragePrice\"]\n",
    "for column in c:\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "# Select data for learning\n",
    "features = c[:-1]+[\"type\",\"region\"]\n",
    "X = df[features]\n",
    "Y = df[\"AveragePrice\"]\n",
    "\n",
    "# Split training into some for training and some for testing\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(\"training split: \", len(X_train), \"; test split: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with Missing Values ########################################################\n",
    "# https://www.kaggle.com/alexisbcook/missing-values\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "def dropColumns(X_train, X_valid):\n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any()]\n",
    "\n",
    "    # Drop columns in training and validation data\n",
    "    reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "    reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "    return reduced_X_train, reduced_X_valid\n",
    "\n",
    "\n",
    "def imputing(X_train, X_valid):\n",
    "    # Imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_valid.columns = X_valid.columns\n",
    "    return imputed_X_train, imputed_X_valid\n",
    "\n",
    "def imputePlus(X_train, X_valid):\n",
    "    X_train_plus = X_train.copy()\n",
    "    X_valid_plus = X_valid.copy()\n",
    "\n",
    "    # Make new columns indicating what will be imputed\n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any()]\n",
    "    for col in cols_with_missing:\n",
    "        X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "        X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "    # Imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "    imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train_plus.columns = X_train_plus.columns\n",
    "    imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "    return imputed_X_train_plus, imputed_X_valid_plus\n",
    "\n",
    "### Dealing with Categorial Variables ##################################################\n",
    "def getCategories(X_train):\n",
    "    # Get list of categorical variables\n",
    "    s = (X_train.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    return object_cols\n",
    "\n",
    "def dropCategories(X_train, X_valid):\n",
    "    object_cols = getCategories(X_train)\n",
    "    drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "    drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "    return drop_X_train, drop_X_valid, Y_train, Y_valid\n",
    "\n",
    "# Label Encoding\n",
    "def labelEncoding(X_train, X_valid):\n",
    "    object_cols = getCategories(X_train)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # Make copy to avoid changing original data \n",
    "    label_X_train = X_train.copy()\n",
    "    label_X_valid = X_valid.copy()\n",
    "\n",
    "    # Apply label encoder to each column with categorical data\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "        label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "    return label_X_train, label_X_valid, Y_train, Y_valid\n",
    "\n",
    "### One Hot Encoding\n",
    "def oneHotEncoding(X_train, X_valid, debug=True):\n",
    "    object_cols = getCategories(X_train)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = X_train.index\n",
    "    OH_cols_valid.index = X_valid.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = X_train.drop(object_cols, axis=1)\n",
    "    num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "    return OH_X_train, OH_X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data!\n",
    "#df.head()\n",
    "X_train, X_test = oneHotEncoding(X_train, X_test)\n",
    "X_train, X_test = imputePlus(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DataBatch</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n      <th>0</th>\n      <th>...</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>51.0</td>\n      <td>174079.23</td>\n      <td>1225.77</td>\n      <td>102134.85</td>\n      <td>3082.60</td>\n      <td>67636.01</td>\n      <td>60761.60</td>\n      <td>6851.61</td>\n      <td>22.80</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32.0</td>\n      <td>24727.46</td>\n      <td>201.46</td>\n      <td>24405.82</td>\n      <td>0.00</td>\n      <td>120.18</td>\n      <td>30.00</td>\n      <td>90.18</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49.0</td>\n      <td>5570915.26</td>\n      <td>2780859.66</td>\n      <td>2108450.36</td>\n      <td>121614.31</td>\n      <td>559990.93</td>\n      <td>520299.26</td>\n      <td>36501.18</td>\n      <td>3190.49</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>35.0</td>\n      <td>634.09</td>\n      <td>228.39</td>\n      <td>24.04</td>\n      <td>0.00</td>\n      <td>381.66</td>\n      <td>381.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11.0</td>\n      <td>2591981.76</td>\n      <td>1014382.60</td>\n      <td>1205553.71</td>\n      <td>74862.42</td>\n      <td>297183.03</td>\n      <td>280841.88</td>\n      <td>13203.35</td>\n      <td>3137.80</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>",
      "text/plain": "   DataBatch  TotalVolume        4046        4225       4770  TotalBags  \\\n0       51.0    174079.23     1225.77   102134.85    3082.60   67636.01   \n1       32.0     24727.46      201.46    24405.82       0.00     120.18   \n2       49.0   5570915.26  2780859.66  2108450.36  121614.31  559990.93   \n3       35.0       634.09      228.39       24.04       0.00     381.66   \n4       11.0   2591981.76  1014382.60  1205553.71   74862.42  297183.03   \n\n   SmallBags  LargeBags  XLargeBags    0  ...   46   47   48   49   50   51  \\\n0   60761.60    6851.61       22.80  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n1      30.00      90.18        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n2  520299.26   36501.18     3190.49  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n3     381.66       0.00        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n4  280841.88   13203.35     3137.80  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n\n    52   53   54   55  \n0  0.0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  0.0  \n2  0.0  0.0  0.0  0.0  \n3  0.0  0.0  0.0  0.0  \n4  0.0  0.0  0.0  0.0  \n\n[5 rows x 65 columns]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci Kit Learn! Scientific Computing library for python\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, presort='deprecated',\n                      random_state=2020, splitter='best')\nRandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=None, oob_score=False,\n                      random_state=2020, verbose=0, warm_start=False)\n"
    },
    {
     "data": {
      "text/plain": "'\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n# Fit only to the training data\\nscaler.fit(Xtrain)\\nXtrain = scaler.transform(Xtrain)\\nXtest = scaler.transform(Xtest)\\n\\nfrom sklearn.neural_network import MLPClassifier\\nscaler = StandardScaler()\\nmodel = MLPClassifier(hidden_layer_sizes=(len(features),len(features),len(features)))\\n'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the regression model we want to use\n",
    "\n",
    "import sklearn.tree as tree\n",
    "models = [\n",
    "    tree.DecisionTreeRegressor(random_state=2020),\n",
    "    ensemble.RandomForestRegressor(random_state=2020, n_estimators=100)\n",
    "]\n",
    "\n",
    "currScore = 0\n",
    "model = None\n",
    "for i in models:\n",
    "    print(i)\n",
    "    i.fit(X_train,Y_train)\n",
    "    score = i.score(X_test,Y_test)\n",
    "    if score > currScore:\n",
    "        model = i\n",
    "        currScore = score\n",
    "\n",
    "# https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html/2\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "scaler = StandardScaler()\n",
    "model = MLPClassifier(hidden_layer_sizes=(len(features),len(features),len(features)))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8231919608917935"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model\n",
    "model.score(X_test, Y_test) #TODO TODO TODO TODO TODO TODO TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data\n",
    "MyDataFrame = pd.read_csv('avocado-test.csv', index_col='id')\n",
    "# Drops missing values \n",
    "#MyDataFrame.dropna()\n",
    "# Convert strings to numbers\n",
    "MyDataFrame[\"Date\"] = pd.to_datetime(MyDataFrame[\"Date\"])\n",
    "c = ['TotalVolume',\"4046\",\"4225\",\"4770\",\"TotalBags\",\"SmallBags\",\"LargeBags\",\"XLargeBags\"]#,\"AveragePrice\"]\n",
    "for column in c:\n",
    "    MyDataFrame[column] = pd.to_numeric(MyDataFrame[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DataBatch</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n      <th>0</th>\n      <th>...</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>21.0</td>\n      <td>28969.34</td>\n      <td>80.77</td>\n      <td>27361.91</td>\n      <td>0.00</td>\n      <td>1526.66</td>\n      <td>1526.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.0</td>\n      <td>322962.89</td>\n      <td>5632.85</td>\n      <td>242365.82</td>\n      <td>2441.56</td>\n      <td>72522.66</td>\n      <td>22942.48</td>\n      <td>46083.51</td>\n      <td>3496.67</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>49.0</td>\n      <td>155334.45</td>\n      <td>3657.79</td>\n      <td>74068.65</td>\n      <td>0.00</td>\n      <td>77608.01</td>\n      <td>27302.29</td>\n      <td>50305.72</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>12.0</td>\n      <td>10231.74</td>\n      <td>341.89</td>\n      <td>8519.00</td>\n      <td>0.00</td>\n      <td>1370.85</td>\n      <td>303.33</td>\n      <td>1067.52</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.0</td>\n      <td>10652.02</td>\n      <td>6905.95</td>\n      <td>39.52</td>\n      <td>0.00</td>\n      <td>3706.55</td>\n      <td>3706.55</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 65 columns</p>\n</div>",
      "text/plain": "   DataBatch  TotalVolume     4046       4225     4770  TotalBags  SmallBags  \\\n0       21.0     28969.34    80.77   27361.91     0.00    1526.66    1526.66   \n1       19.0    322962.89  5632.85  242365.82  2441.56   72522.66   22942.48   \n2       49.0    155334.45  3657.79   74068.65     0.00   77608.01   27302.29   \n3       12.0     10231.74   341.89    8519.00     0.00    1370.85     303.33   \n4       10.0     10652.02  6905.95      39.52     0.00    3706.55    3706.55   \n\n   LargeBags  XLargeBags    0  ...   46   47   48   49   50   51   52   53  \\\n0       0.00        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1   46083.51     3496.67  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2   50305.72        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3    1067.52        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4       0.00        0.00  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n    54   55  \n0  0.0  0.0  \n1  0.0  0.0  \n2  0.0  0.0  \n3  0.0  0.0  \n4  0.0  0.0  \n\n[5 rows x 65 columns]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "XforPredictions =  MyDataFrame[features]#None #TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "\n",
    "\n",
    "X_train, X_test = oneHotEncoding(X, XforPredictions)\n",
    "X_train, X_test = imputePlus(X_train ,X_test)\n",
    "\n",
    "#XforPredictions.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predictions: [1.8924 1.511  1.3193 ... 1.4259 1.923  1.6556]\n"
    }
   ],
   "source": [
    "# Make Predictions!\n",
    "#model.fit(X,Y)\n",
    "yPredictions = model.predict(X_test)# None #TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "print(\"Predictions:\", yPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "id  AveragePrice\n0        0        1.8924\n1        1        1.5110\n2        2        1.3193\n3        3        1.6310\n4        4        1.4370\n...    ...           ...\n5470  5470        1.6974\n5471  5471        1.3266\n5472  5472        1.4259\n5473  5473        1.9230\n5474  5474        1.6556\n\n[5475 rows x 2 columns]\n"
    }
   ],
   "source": [
    "# Save to CSV File!\n",
    "XforPredictions =  MyDataFrame[features]\n",
    "output = pd.DataFrame({'id': XforPredictions.index, 'AveragePrice': yPredictions})\n",
    "output.to_csv('avocado-submission.csv', index=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloaded pyctfsglib.py: True\n"
    }
   ],
   "source": [
    "# Download CTFSG Grader Libraries\n",
    "import urllib.request, os\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/alttablabs/ctfsg-utils/master/pyctfsglib.py', './pyctfsglib.py')\n",
    "print('Downloaded pyctfsglib.py:', 'pyctfsglib.py' in os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DSGraderClient: Successfully Connected!\n[SERVER] MOTD: CHECK your USER_TOKEN and GRADER_URL HTTP address! I'm AVOCADO_PRICE TEST_GRADER_1\n"
    }
   ],
   "source": [
    "# Connect to graders\n",
    "import pyctfsglib as ctfsg\n",
    "import random\n",
    "\n",
    "USER_TOKEN = \"MXhtGfdjdsUfiEKTHHEuVGohZESBdMiHrFkmYqNqIFfcWOHGvcubvHJvnxpAqRMh\" # You need to fill this up\n",
    "GRADER_URL = random.choice([\n",
    "\"http://challenges.csdc20t.ctf.sg:30001/\", \"http://challenges.csdc20t.ctf.sg:30002/\"\n",
    "])\n",
    "grader = ctfsg.DSGraderClient(GRADER_URL, USER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ProofOfWork Challenge =>  ('CTFSGRBef83136251e54cf2ebec5f8bc7c38b74', 22)\nProofOfWork Answer Found! =>  3983024\n"
    },
    {
     "data": {
      "text/plain": "'{\"challenge\":{\"name\":\"Avocado Prices\"},\"id\":\"ck8n1ufyy70qn0702thqqzz3j\",\"status\":\"PARTIALLY_CORRECT\",\"multiplier\":0.8352,\"submittedBy\":{\"username\":\"nyjc-1\"},\"createdAt\":\"2020-04-05T12:56:02Z\"}'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.submitFile('avocado-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}